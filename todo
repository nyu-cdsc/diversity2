- working config/runner
  - finally need to separate object instantiation from the config
    - Param objects for instance will have response objects, but in the config they won't have 'used' properties on them
    - just make it optional, still separate. make the object _create_ functions in stimuliservice for the generator
- working event flow
  - response working
    - params give us response object
    - we set switch of 'used' to true whenever one checked and send the entire object up to parent
      - rather than just sending the specific response
    - FOR NOW, just replace that single object of each type that will later be marked as the response type
    - or the values-carrying type. each stimuli has one of these? it's meeting an interface or just a role
    - at the end of it, could just have a marker of 'responder'

- get conditional choices working through generator and back

- yaml
- export
- validation and error handling
- stricter typing, docblocks, docs

flow is as events --
  stimuli -> appComponent/response aggregator/Frame -> <- service (in reverse)
response sent stimuli->parent, it decides what to let through based on Control settings/param/whatever the child tells it

TODO - move all of this into specific sections rather than global




DEPLOY:
  - query/store GKE/EKS account credentials
  - direct api to GKE/EKS to spin up nodes -- and create buckets - no TF?
    - make new resource, store reference to resource
  - direct api to send over kube pods
  - pull down key for auth
    - look into how packer does it. without that key I can't get into the instance flat-out, as ssh is the only way in/out
    - here it would be an api, can use the same key for both?
    - look into how it's done on kube
    - make sure it's all in gitignore/stored outside the directory
    - could also put a passphrase on it just in case

    - could also store it in bucket accessible by aws/eks lab member accounts
    - or just that lab account, and administrator account

    - if credentials are never stored, but rather requested every time and instance (api) data accessible via aws/eks/oauth account..
      - that could be better. much better
      - the key used for creation could have lab-known passphrase and be on bucket accessible to just their accounts
      - panda and this login should be oauth/sso anyway. even if this is just at shell interface stage
PULL:
  - read reference to resource
  - connect over some api call rather than ssh, use key to get to it
  - .. figure it out
  - could also just store it in bucket that lab members can access
DESTROY:
  - first PULL all data down to local archive
  - reverse of DEPLOY

LOG -- log the ENTIRE sequence, essentially log the Frames that were traversed, the final tree that the participant experienced,
so it is very clear what occurred
  - could literally print tree mapped to responses (would be nifty feature) in addition to logging the Frame they were in, on
    the Response/eventual csv row
  - could make super nice svg render of it (and text - strategy pattern (w/-mac.txt and -pc.txt for newline variance))
    - or just switch that asks for svg or txt.
    - or just output all into the final zip - csv, svg, txt files
  - and that's what is output from export button in the app, too
  - so just the csv, txt and svg renderers on Response service export()





later:
  - this respository should probably be treated as a dependency, and their projects should be nothing other than:
    package.json
    runsheet.yaml
    assets/

    - and then npm run
      - init, test/serve/develop, deploy, pull, destroy, etc.
      - deploy/pull/destroy just forward to the actual toku package.json call, none of that is in this scaffold repo
      - so it checks and runs init etc. first before actually calling those internally
